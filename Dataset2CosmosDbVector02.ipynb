{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Load beir dataset\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "data_path = \"datasets/hotpotqa\"\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(f'Corpus size: {len(corpus)}')\n",
    "# Take the first n items from the corpus dictionary, which is not part of the qrels_select\n",
    "n = 5\n",
    "corpus_sample = dict(list(corpus.items())[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from azure.cosmos import CosmosClient, PartitionKey, exceptions\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Cosmos client\n",
    "connection_string = os.getenv('COSMOSDB_CONN_STR') \n",
    "client = CosmosClient.from_connection_string(connection_string)\n",
    "\n",
    "# Define the database and container\n",
    "database_name = os.getenv('COSMOSDB_DB_NAME') \n",
    "container_name = os.getenv('COSMOSDB_CONTAINER_NAME') \n",
    "database = client.get_database_client(database_name)\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.CRITICAL)  # set urllib3 logging level to CRITICAL\n",
    "logging.getLogger(\"azure\").setLevel(logging.CRITICAL)  # set urllib3 logging level to CRITICAL\n",
    "\n",
    "# Define indexing policy and vector embedding policy if needed\n",
    "vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [ \n",
    "        { \n",
    "            \"path\": \"/vectorized_text\", \n",
    "            \"dataType\": \"float32\", \n",
    "            \"distanceFunction\": \"euclidean\", \n",
    "            \"dimensions\":  1536\n",
    "        }\n",
    "    ] \n",
    "}\n",
    "\n",
    "indexing_policy = {\n",
    "    \"indexingMode\": \"consistent\",\n",
    "    \"automatic\": True,\n",
    "    \"includedPaths\": [\n",
    "        {\n",
    "            \"path\": \"/*\"\n",
    "        }\n",
    "    ],\n",
    "    \"excludedPaths\": [\n",
    "        {\n",
    "            \"path\": \"/\\\"_etag\\\"/?\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/vectorized_text/*\"\n",
    "        }\n",
    "    ],\n",
    "    \"fullTextIndexes\": [],\n",
    "    \"vectorIndexes\": [\n",
    "        {\n",
    "        \"path\": \"/vectorized_text\",\n",
    "        \"type\": \"diskANN\",\n",
    "        \"quantizationByteSize\": 128,\n",
    "        \"IndexingSearchListSize\": 100\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create container if not exists\n",
    "try:\n",
    "    container = database.create_container_if_not_exists(\n",
    "        id=container_name,\n",
    "        partition_key=PartitionKey(path='/id'),\n",
    "        indexing_policy=indexing_policy,\n",
    "        vector_embedding_policy=vector_embedding_policy\n",
    "    )\n",
    "    print(f'Container with id \\'{container_name}\\' created')\n",
    "except exceptions.CosmosHttpResponseError as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos.aio import CosmosClient\n",
    "from azure.cosmos.exceptions import CosmosHttpResponseError\n",
    "\n",
    "async def insert_documents_to_cosmosdb(connection_string: str, database_name: str, container_name: str, documents: list):\n",
    "    \"\"\"\n",
    "    Insert a list of JSON documents into Cosmos DB asynchronously.\n",
    "\n",
    "    :param connection_string: Cosmos DB connection string for authentication.\n",
    "    :param database_name: Name of the Cosmos DB database.\n",
    "    :param container_name: Name of the Cosmos DB container.\n",
    "    :param documents: List of JSON documents to insert.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a Cosmos DB client\n",
    "        async with CosmosClient.from_connection_string(connection_string) as client:\n",
    "            # Get the database and container\n",
    "            database = client.get_database_client(database_name)\n",
    "            container = database.get_container_client(container_name)\n",
    "\n",
    "            # Insert documents asynchronously\n",
    "            tasks = []\n",
    "            for doc in documents:\n",
    "                tasks.append(container.upsert_item(doc))  # Use upsert to insert or update\n",
    "\n",
    "            # Wait for all tasks to complete\n",
    "            await asyncio.gather(*tasks)\n",
    "            print(f\"Successfully inserted {len(documents)} documents into Cosmos DB.\")\n",
    "\n",
    "    except CosmosHttpResponseError as e:\n",
    "        print(f\"An error occurred: {e.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from openai import AsyncAzureOpenAI\n",
    "import time\n",
    "\n",
    "# Initialize the Azure OpenAI client\n",
    "api_key = os.getenv('AOAI_API_KEY')\n",
    "azure_endpoint = os.getenv('AOAI_ENDPOINT')\n",
    "\n",
    "aclient = AsyncAzureOpenAI(api_key=api_key,\n",
    "api_version=\"2024-12-01-preview\",\n",
    "azure_endpoint=azure_endpoint,\n",
    "max_retries=5)\n",
    "\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.CRITICAL)  # set urllib3 logging level to CRITICAL\n",
    "logging.getLogger(\"openai\").setLevel(logging.CRITICAL)  # set urllib3 logging level to CRITICAL\n",
    "logging.getLogger(\"httpx\").setLevel(logging.CRITICAL)  # set urllib3 logging level to CRITICAL\n",
    "logging.getLogger(\"azure.cosmos\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"azure\").setLevel(logging.CRITICAL)\n",
    "\n",
    "model_name = \"text-embedding-3-small\"\n",
    "\n",
    "# Semaphore to limit concurrency\n",
    "semaphore = asyncio.Semaphore(500)  # Adjust the limit as needed\n",
    "\n",
    "from azure.core.exceptions import ServiceResponseError\n",
    "\n",
    "async def vectorize_text(text: str):\n",
    "    max_retries = 5  # Increase retries for robustness\n",
    "    retry_delay = 2  # seconds\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            async with semaphore:  # Limit the number of concurrent tasks\n",
    "                response = await aclient.embeddings.create(input=text, model=model_name)\n",
    "                data = response.data\n",
    "                if data:\n",
    "                    return data[0].embedding  # Ensure embedding is JSON-serializable\n",
    "                return []\n",
    "        except ServiceResponseError as e:\n",
    "            print(f\"ServiceResponseError: {e}. Attempt {attempt + 1} of {max_retries}. Retrying in {retry_delay} seconds...\")\n",
    "            if attempt < max_retries - 1:\n",
    "                await asyncio.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Max retries reached for ServiceResponseError. Raising exception.\")\n",
    "                raise e\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Attempt {attempt + 1} failed due to {e}. Retrying in {retry_delay} seconds...\")\n",
    "                await asyncio.sleep(retry_delay)\n",
    "            else:\n",
    "                print(\"Max retries reached. Raising exception.\")\n",
    "                raise e\n",
    "\n",
    "import time\n",
    "# Wait to avoid rate limiting by embedding service\n",
    "wait_time = 0.01\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Corpues to vectorize\n",
    "#corpus_to_vectorize = corpus_sample\n",
    "print(f'Length of corpus: {len(corpus)}')\n",
    "\n",
    "# Process in batches \n",
    "batch_size = 500\n",
    "json_array = []\n",
    "\n",
    "#keys_values = list(corpus_to_vectorize.items()) # to load only the sample\n",
    "starting_index = 291000\n",
    "corpus_to_vectorize = dict(list(corpus.items())[starting_index:])\n",
    "#corpus_to_vectorize = corpus_sample\n",
    "print(f'Length of corpus to vectorize: {len(corpus_to_vectorize)}')\n",
    "\n",
    "keys_values = list(corpus_to_vectorize.items()) # to load everything\n",
    "for i in range(starting_index, len(keys_values), batch_size):\n",
    "    batch = keys_values[i:i + batch_size]\n",
    "    tasks = [vectorize_text(f\"Tile: {value['title']}, Text: {value['text']}\") for key, value in batch]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for (key, value), embedding in zip(batch, results):\n",
    "        if embedding:  # ...existing check logic...\n",
    "            json_array.append({\n",
    "                'id': key,\n",
    "                'text': value['text'],\n",
    "                'title': value['title'],\n",
    "                'vectorized_text': embedding\n",
    "            })\n",
    "    await insert_documents_to_cosmosdb(connection_string, database_name, container_name, json_array)\n",
    "    json_array = []\n",
    "    print(f'Batch {i // batch_size + 1} completed. Processed {len(batch)} items in {time.time() - start_time} seconds')\n",
    "\n",
    "\n",
    "print(f'All documents has been vectorized. Length of the JSON array: {len(json_array)}')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#logging.getLogger(\"azure.cosmos\").setLevel(logging.CRITICAL)\n",
    "#logging.getLogger(\"openai\").setLevel(logging.CRITICAL)\n",
    "#logging.getLogger(\"httpcore\").setLevel(logging.CRITICAL)\n",
    "# Query to get the count of items\n",
    "query = \"SELECT VALUE COUNT(1) FROM c\"\n",
    "\n",
    "# Execute the query\n",
    "result = list(container.query_items(query=query, enable_cross_partition_query=True))\n",
    "\n",
    "# The result will be a list with a single value (the count)\n",
    "for r in result:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
